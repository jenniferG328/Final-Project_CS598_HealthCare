{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "**Please find the public repo at: https://github.com/jenniferG328/Final-Project_CS598_HealthCare**\n",
    "\n",
    "## Background of the Problem\n",
    "#### Context: \n",
    "Advances in deep learning have enhanced automated medical image analysis, but existing techniques face high computational requirements and performance drops with reduced batch sizes or training epochs.\n",
    "\n",
    "#### Problem Type: \n",
    "The paper addresses issues in medical image analysis, particularly focusing on self-supervised learning approaches for processing label-free images efficiently.\n",
    "\n",
    "#### Importance: \n",
    "\n",
    "Solving this problem is crucial because data labeling in medical imaging is expensive and time-consuming, and often data is scarce, especially for emerging diseases like certain autoimmune conditions.\n",
    "\n",
    "#### Difficulty: \n",
    "\n",
    "Challenges include minimal data availability, the need for domain-specific knowledge for labeling, patient privacy issues, and an incomplete understanding of diseases.\n",
    "\n",
    "#### State-of-the-Art Methods and Effectiveness\n",
    "\n",
    "Current Techniques: The prevailing methods in medical image analysis primarily rely on self-supervised learning frameworks utilizing either Convolutional Neural Networks (CNNs) or Transformers. These techniques are heavily dependent on extensive datasets and large batch sizes.\n",
    "\n",
    "Performance Challenges: A notable limitation of these existing approaches is their significant reduction in performance when the conditions of large datasets and batch sizes are not met. This issue becomes more pronounced with constrained computational resources.\n",
    "\n",
    "Computational Demands: Current state-of-the-art methods require considerable computational power, which poses a barrier to their application, especially in settings with limited resources. Such extensive computational requirements limit the practical accessibility of these advanced techniques.\n",
    "\n",
    "## Paper Explanation\n",
    "#### Proposal: \n",
    "The paper introduces Cross Architectural Self-Supervision (CASS), a novel method combining CNNs and Transformers in a self-supervised learning setting. It addresses the challenges of limited data and computational resources in medical image analysis.\n",
    "\n",
    "#### Innovations: \n",
    "CASS leverages both CNN and Transformer architectures simultaneously, improving robustness to changes in batch size and training epochs and reducing computational requirements.\n",
    "\n",
    "#### Performance: \n",
    "Demonstrated improvements across four medical datasets in terms of F1 Score and Recall, using less labeled data and significantly less training time compared to existing methods.\n",
    "\n",
    "#### Contribution: \n",
    "CASS represents a significant step in self-supervised learning for medical image analysis, especially beneficial for emerging diseases with limited data. It stands out in efficiency, effectiveness, and adaptability to resource constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scope of Reproducibility \n",
    "\n",
    "**Hypotheses to be Tested**\n",
    "\n",
    "-  Hypotheses 1  \n",
    "Reproduced CASS-trained models outperform existing self-supervised learning methods in terms of\n",
    "accuracy and efficiency on healthcare tasks shown in the paper, i.e., disease cell classification, brain\n",
    "tumor classification, and skin lesion classification.\n",
    "\n",
    "-  Hypotheses 2  \n",
    "Reproduced CASS demonstrates greater robustness to variations in batch size and pretraining epochs\n",
    "compared to current methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology:\n",
    "\n",
    "## Data\n",
    "\n",
    "#### Data descriptions\n",
    "\n",
    "Autoimmune Diseases Biopsy Slides Dataset: This dataset includes 198 TIFF images from muscle biopsies of dermatomyositis patients. These slides are stained with different proteins to help diagnose dermatomyositis, a type of autoimmune disease. The dataset involves multi-label classification for different cell classes, such as TFH-1, TFH-217, TFH-Like B cells, and others. The images are consistent in size, measuring 352 by 469 pixels in RGB format .\n",
    "\n",
    "Dermofit Dataset: Comprising 1300 normal RGB images captured indoors with an SLR camera and ring lightning, this dataset categorizes images into 10 classes associated with skin lesions and conditions. The images vary in size, ranging from 205×205 to 1020×1020 pixels, with no two images being the same size. The dataset's primary task is multi-class classification .\n",
    "\n",
    "Brain Tumor MRI Dataset: This dataset includes 7022 images of human brain MRIs, classified into four categories: glioma, meningioma, no tumor, and pituitary. The images vary in size from 512×512 to 219×234 pixels. The dataset's source is a combination of different datasets and includes 5712 images for training and 1310 for testing .\n",
    "\n",
    "\n",
    "Using Brain Tumor MRI Dataset for a demo in this report, switch the  name to run experiments on other datasets.\n",
    "\n",
    "#### Data Access\n",
    "\n",
    "Data will be accessed through collaborations with healthcare institutions and the use of publicly available medical datasets.\n",
    "\n",
    "We could refer to the following links for the datasets.\n",
    "\n",
    "(1) https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset\n",
    "\n",
    "(2) https://homepages.inf.ed.ac.uk/rbf/DERMOFIT/datasets.htm\n",
    "\n",
    "(3) https://challenge.isic-archive.com/data/\n",
    "\n",
    "(4) https://github.com/pranavsinghps1/DEDL/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold\n",
    "import math\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torchcontrib.optim import SWA\n",
    "from torchmetrics import Metric\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing and augmentation\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = ImageFolder('brain_tumor/Training', transform=data_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "val_dataset = ImageFolder('brain_tumor/Testing', transform=data_transforms)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "# Load a batch of images and labels for visualization\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# Convert images to numpy arrays and denormalize\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "images = (images.numpy().transpose((0, 2, 3, 1)) * std + mean).clip(0, 1)\n",
    "\n",
    "# Create a grid of images\n",
    "num_images = len(images)\n",
    "rows = int(np.ceil(num_images / 4))\n",
    "fig, axes = plt.subplots(rows, 4, figsize=(15, 15))\n",
    "\n",
    "# Plot images with labels\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < num_images:\n",
    "        ax.imshow(images[i])\n",
    "        ax.set_title(f'Label: {train_dataset.classes[labels[i]]}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "\n",
    "\n",
    "#### Model Architecture\n",
    "Layers and Types: CASS employs a dual architecture comprising a Convolutional Neural Network (CNN) and a Vision Transformer (ViT). Specific examples mentioned are ResNet-50 for the CNN and ViT Base/16 for the Transformer.\n",
    "Activation Functions: The paper does not explicitly mention the activation functions used in the model architectures, but standard practices for ResNet and ViT typically involve ReLU and GELU activations, respectively.\n",
    "#### Training Objectives\n",
    "Loss Function: The model uses a cosine similarity-based loss function and focal loss, specifically designed for comparing the logits outputs from the CNN and ViT.\n",
    "#### Optimizer: \n",
    "The training employs Adam optimizer with a learning rate of 1e-3 for both the CNN and ViT, along with stochastic weight averaging (SWA) for optimization.\n",
    "\n",
    "#### Weight of Each Loss Term: \n",
    "\n",
    "Details about the weight of each loss term are not explicitly mentioned. However, the loss is computed as the mean value of all elements in the tensor derived from the cosine similarity calculation between the outputs of the CNN and ViT.\n",
    "#### Others\n",
    "Pretraining: CASS is based on self-supervised learning, and it is mentioned that the models were trained from ImageNet initialization for 100 epochs.\n",
    "\n",
    "Training Process: The paper describes the training process, mentioning that they use the same set of augmentations in self-supervised training and also detail the hyper-parameters for both the self-supervised and supervised training phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define Focal-Loss\n",
    "\"\"\"\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    The focal loss for fighting against class-imbalance\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1e-12  # prevent training from Nan-loss error\n",
    "        self.cls_weights = torch.tensor([CFG.cls_weight],dtype=torch.float, requires_grad=False, device=CFG.device)\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        logits & target should be tensors with shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        probs = torch.sigmoid(logits)\n",
    "        one_subtract_probs = 1.0 - probs\n",
    "        # add epsilon\n",
    "        probs_new = probs + self.epsilon\n",
    "        one_subtract_probs_new = one_subtract_probs + self.epsilon\n",
    "        # calculate focal loss\n",
    "        log_pt = target * torch.log(probs_new) + (1.0 - target) * torch.log(one_subtract_probs_new)\n",
    "        pt = torch.exp(log_pt)\n",
    "        focal_loss = -1.0 * (self.alpha * (1 - pt) ** self.gamma) * log_pt\n",
    "        focal_loss = focal_loss * self.cls_weights\n",
    "        return torch.mean(focal_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # label_num2str = {'0': 'glioma', '1': 'meningioma', '2': 'notumor', '3': 'pituitary'}\n",
    "    label_num2str = {0: 'glioma', 1: 'meningioma', 2: 'notumor', 3: 'pituitary'}\n",
    "    label_str2num = {'glioma': '0', 'meningioma': '1', 'notumor': '2','pituitary': '3'}\n",
    "    fl_alpha = 1.0  # alpha of focal_loss\n",
    "    fl_gamma = 2.0  # gamma of focal_loss\n",
    "    cls_weight =  [0.5, 0.5, 0.5, 0.5]\n",
    "    cnn_name='resnet50'\n",
    "    vit_name='vit_base_patch16_224'\n",
    "    seed = 77\n",
    "    num_classes = 4\n",
    "    batch_size = 16\n",
    "    t_max = 16\n",
    "    lr = 1e-3\n",
    "    min_lr = 1e-6\n",
    "    n_fold = 6\n",
    "    num_workers = 8\n",
    "    gpu_idx = 0\n",
    "    device = torch.device(f'cuda:{gpu_idx}' if torch.cuda.is_available() else 'cpu')\n",
    "    gpu_list = [gpu_idx]\n",
    "    CNN = True\n",
    "    VIT = False\n",
    "    \n",
    "cfg=CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = timm.create_model(cfg.cnn_name, pretrained=True)\n",
    "model_vit = timm.create_model(cfg.vit_name, pretrained=True)\n",
    "model_cnn.to(device)\n",
    "model_vit.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "#### Computational requirements and GPU Utilization: \n",
    "The model was trained on a single NVIDIA RTX8000 GPU, which significantly facilitated a reduced training time. We use NVIDIA RXT2080ti to reproduce all the experiments.\n",
    "\n",
    "#### Training Time Efficiency: \n",
    "The paper highlights that CASS took substantially less time compared to the DINO method for self-supervised training. For example, on the Autoimmune Diseases Biopsy Slides dataset, CASS required only 21 minutes compared to DINO's 1 hour 13 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssl_train_model(train_loader,model_vit,criterion_vit,optimizer_vit,scheduler_vit,model_cnn,criterion_cnn,optimizer_cnn,scheduler_cnn,num_epochs):\n",
    "    writer = SummaryWriter()\n",
    "    phase = 'train'\n",
    "    model_cnn.train()\n",
    "    model_vit.train()\n",
    "    f1_score_cnn=0\n",
    "    f1_score_vit=0\n",
    "    for i in tqdm(range(num_epochs)):\n",
    "        with torch.set_grad_enabled(phase == 'train'):\n",
    "            for img,_ in tqdm(train_loader):\n",
    "                f1_score_cnn=0\n",
    "                f1_score_vit=0\n",
    "                img = img.to(device)\n",
    "                pred_vit = model_vit(img)\n",
    "                pred_cnn = model_cnn(img)\n",
    "                model_sim_loss=loss_fn(pred_vit,pred_cnn)\n",
    "                loss = model_sim_loss.mean()\n",
    "                loss.backward()\n",
    "                optimizer_cnn.step()\n",
    "                optimizer_vit.step()\n",
    "                scheduler_cnn.step()\n",
    "                scheduler_vit.step()\n",
    "            print('For -',i,'Loss:',loss) \n",
    "            writer.add_scalar(\"Self-Supervised Loss/train\", loss, i)\n",
    "    writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_cnn = SWA(torch.optim.Adam(model_cnn.parameters(), lr= 1e-3))\n",
    "optimizer_vit = SWA(torch.optim.Adam(model_vit.parameters(), lr= 1e-3))\n",
    "scheduler_cnn = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_cnn,\n",
    "                                                                    T_max=16,\n",
    "                                                                    eta_min=1e-6)\n",
    "scheduler_vit = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_vit,\n",
    "                                                                    T_max=16,\n",
    "                                                                    eta_min=1e-6)\n",
    "\n",
    "\n",
    "fl_alpha = 1.0  # alpha of focal_loss\n",
    "fl_gamma = 2.0  # gamma of focal_loss\n",
    "cls_weight = [0.5, 0.5, 0.5, 0.5]\n",
    "criterion_vit = FocalLoss(fl_alpha, fl_gamma)\n",
    "criterion_cnn = FocalLoss(fl_alpha, fl_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(x, y):\n",
    "    x =  torch.nn.functional.normalize(x, dim=-1, p=2)\n",
    "    y =  torch.nn.functional.normalize(y, dim=-1, p=2)\n",
    "    return 2 - 2 * (x * y).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl_train_model(train_loader,model_vit,criterion_vit,optimizer_vit,scheduler_vit,model_cnn,criterion_cnn,optimizer_cnn,scheduler_cnn,num_epochs=10)\n",
    "    #Saving SSL Models\n",
    "print('Saving Cov-T')\n",
    "    \n",
    "torch.save(model_cnn,'./cass-r50.pt')\n",
    "torch.save(model_vit,'./cass-vit.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream Tuning and Evaluation\n",
    "\n",
    "#### Metrics Descriptions in CASS Paper\n",
    "\n",
    "- **Metric Used**: F1 Score\n",
    "- **Definition**: \n",
    "  - F1 Score is calculated as `F1 = 2 × (Precision × Recall) / (Precision + Recall)`, \n",
    "    where Precision is the ratio of true positive predictions to the total positive predictions,\n",
    "    and Recall is the ratio of true positive predictions to the total actual positives.\n",
    "- **Reason for Choice**: \n",
    "  - Selected based on previous work or as defined by the dataset provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define F1 score metric\n",
    "\"\"\"\n",
    "class MyF1Score(Metric):\n",
    "    def __init__(self, cfg, threshold: float = 0.5, dist_sync_on_step=False):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        self.cfg = cfg\n",
    "        self.threshold = threshold\n",
    "        self.add_state(\"tp\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"fp\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"fn\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        assert preds.shape == target.shape\n",
    "        preds_str_batch = self.num_to_str(torch.sigmoid(preds))\n",
    "        target_str_batch = self.num_to_str(target)\n",
    "        tp, fp, fn = 0, 0, 0\n",
    "        for pred_str_list, target_str_list in zip(preds_str_batch, target_str_batch):\n",
    "            for pred_str in pred_str_list:\n",
    "                if pred_str in target_str_list:\n",
    "                    tp += 1\n",
    "                if pred_str not in target_str_list:\n",
    "                    fp += 1\n",
    "\n",
    "            for target_str in target_str_list:\n",
    "                if target_str not in pred_str_list:\n",
    "                    fn += 1\n",
    "        self.tp += tp\n",
    "        self.fp += fp\n",
    "        self.fn += fn\n",
    "\n",
    "    def compute(self):\n",
    "        f1 = 2.0 * self.tp / (2.0 * self.tp + self.fn + self.fp)\n",
    "        return f1\n",
    "    \n",
    "    def num_to_str(self, ts: torch.Tensor) -> list:\n",
    "        batch_bool_list = (ts > self.threshold).detach().cpu().numpy().tolist()\n",
    "        batch_str_list = []\n",
    "        for one_sample_bool in batch_bool_list:\n",
    "            lb_str_list = [self.cfg.label_num2str[lb_idx] for lb_idx, bool_val in enumerate(one_sample_bool) if bool_val]\n",
    "            batch_str_list.append(lb_str_list)\n",
    "        return batch_str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.CNN:\n",
    "\n",
    "    model_vit=torch.load('./cass-vit.pt')\n",
    "    model_cnn=torch.load('./cass-r50.pt')\n",
    "    last_loss=math.inf\n",
    "    val_loss_arr=[]\n",
    "    train_loss_arr=[]\n",
    "    counter=0\n",
    "        \n",
    "    model_cnn.to(device)\n",
    "    model_vit.to(device)\n",
    "    print('*'*10)\n",
    "    \n",
    "        \n",
    "    #Train Correspong Supervised CNN\n",
    "    print('Fine tunning Cov-T')\n",
    "    writer = SummaryWriter()\n",
    "    model_cnn.fc=nn.Linear(in_features=2048, out_features=4, bias=True)\n",
    "    criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
    "    metric = MyF1Score(cfg)\n",
    "    val_metric=MyF1Score(cfg)\n",
    "    optimizer = torch.optim.Adam(model_cnn.parameters(), lr = 3e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.t_max,eta_min=cfg.min_lr)\n",
    "    model_cnn.train()\n",
    "    from torch.autograd import Variable\n",
    "    best=0\n",
    "    best_val=0\n",
    "    for epoch in tqdm(range(200)):\n",
    "        total_loss = 0\n",
    "        for images, label in train_loader:\n",
    "            model_cnn.train()\n",
    "            images = images.to(device)\n",
    "            label = label.to(device)\n",
    "            model_cnn.to(device)\n",
    "            pred_ts=model_cnn(images)\n",
    "            label_one_hot = torch.nn.functional.one_hot(label, num_classes=4).float()\n",
    "            # print(pred_ts.shape, label.shape,label)\n",
    "            loss = criterion(pred_ts, label_one_hot)\n",
    "            score = metric(pred_ts,label_one_hot)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.detach()\n",
    "        avg_loss=total_loss/ len(train_loader)\n",
    "        train_score=metric.compute()\n",
    "        logs = {'train_loss': avg_loss, 'train_f1': train_score, 'lr': optimizer.param_groups[0]['lr']}\n",
    "        writer.add_scalar(\"CNN Supervised Loss/train\", loss, epoch)\n",
    "        writer.add_scalar(\"CNN Supervised F1/train\", train_score, epoch)\n",
    "        print(logs)\n",
    "        if best < train_score:\n",
    "            best=train_score\n",
    "            model_cnn.eval()\n",
    "            total_loss = 0\n",
    "            for images,label in val_loader:\n",
    "                images = images.to(device)\n",
    "                label = label.to(device)\n",
    "                model_cnn.to(device)\n",
    "                pred_ts=model_cnn(images)\n",
    "                label_one_hot = torch.nn.functional.one_hot(label, num_classes=4).float()\n",
    "                score_val = val_metric(pred_ts, label_one_hot)\n",
    "                val_loss = criterion(pred_ts, label_one_hot)\n",
    "                total_loss += val_loss.detach()\n",
    "            avg_loss=total_loss/ len(train_loader)   \n",
    "            print('Val Loss:',avg_loss)\n",
    "            val_score=val_metric.compute()\n",
    "            print('CNN Validation Score:',val_score)\n",
    "            writer.add_scalar(\"CNN Supervised F1/Validation\", val_score, epoch)\n",
    "            if avg_loss > last_loss:\n",
    "                counter+=1\n",
    "            else:\n",
    "                counter=0\n",
    "                    \n",
    "            last_loss = avg_loss\n",
    "            if counter > 5:\n",
    "                print('Early Stopping!')\n",
    "                break\n",
    "            else:\n",
    "                if val_score > best_val:\n",
    "                    best_val=val_score\n",
    "                    print('Saving')\n",
    "                    torch.save(model_cnn,\n",
    "                        './cass-r50-tuned.pt')\n",
    "\n",
    "\n",
    "if cfg.VIT:\n",
    "    print('Fine tunning Cov-T')\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    model_vit=torch.load('./cass-vit.pt')\n",
    "    model_cnn=torch.load('./cass-r50.pt')\n",
    "    last_loss=math.inf\n",
    "    val_loss_arr=[]\n",
    "    train_loss_arr=[]\n",
    "    counter=0\n",
    "        \n",
    "    model_cnn.to(device)\n",
    "    model_vit.to(device)\n",
    "    print('*'*10)\n",
    "    \n",
    "    writer.flush()\n",
    "    last_loss=999999999\n",
    "    val_loss_arr=[]\n",
    "    train_loss_arr=[]\n",
    "    counter=0\n",
    "    # Training the Corresponding ViT\n",
    "    model_vit.head=nn.Linear(in_features=768, out_features=4, bias=True)\n",
    "    criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
    "    metric = MyF1Score(cfg)\n",
    "    optimizer = torch.optim.Adam(model_vit.parameters(), lr = 3e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.t_max,eta_min=cfg.min_lr)\n",
    "    model_vit.train()\n",
    "    val_metric=MyF1Score(cfg)\n",
    "    writer = SummaryWriter()\n",
    "    from torch.autograd import Variable\n",
    "    best=0\n",
    "    best_val=0\n",
    "    for epoch in tqdm(range(200)):\n",
    "        total_loss = 0\n",
    "        for images,label in train_loader:\n",
    "            model_vit.train()\n",
    "            images = images.to(device)\n",
    "            label = label.to(device)\n",
    "            label = torch.nn.functional.one_hot(label, num_classes=4).float().to(device)\n",
    "            model_vit.to(device)\n",
    "            pred_ts=model_vit(images)\n",
    "            loss = criterion(pred_ts, label)\n",
    "            score = metric(pred_ts,label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.detach()\n",
    "        avg_loss=total_loss/ len(train_loader)\n",
    "        train_score=metric.compute()\n",
    "        logs = {'train_loss': loss, 'train_f1': train_score, 'lr': optimizer.param_groups[0]['lr']}\n",
    "        writer.add_scalar(\"ViT Supervised Loss/train\", loss, epoch)\n",
    "        writer.add_scalar(\"ViT Supervised F1/train\", train_score, epoch)\n",
    "        print(logs)\n",
    "        if best < train_score:\n",
    "            best=train_score\n",
    "            model_vit.eval()\n",
    "            total_loss = 0\n",
    "            for images,label in val_loader:\n",
    "                images = images.to(device)\n",
    "                label = label.to(device)\n",
    "                label = torch.nn.functional.one_hot(label, num_classes=4).float().to(device)\n",
    "                model_vit.to(device)\n",
    "                pred_ts=model_vit(images)\n",
    "                score_val = val_metric(pred_ts,label)\n",
    "                val_loss = criterion(pred_ts, label)\n",
    "                total_loss += val_loss.detach()\n",
    "            avg_loss=total_loss/ len(train_loader)\n",
    "            val_score=val_metric.compute()\n",
    "            print('ViT Validation Score:',val_score)\n",
    "            print('Val Loss:',avg_loss)\n",
    "            writer.add_scalar(\"ViT Supervised F1/Validation\", val_score, epoch)\n",
    "            if avg_loss > last_loss:\n",
    "                counter+=1\n",
    "            else:\n",
    "                counter=0\n",
    "                    \n",
    "            last_loss = avg_loss\n",
    "            if counter > 5:\n",
    "                print('Early Stopping!')\n",
    "                break\n",
    "            else:\n",
    "                if val_score > best_val:\n",
    "                    best_val=val_score\n",
    "                    print('Saving')\n",
    "                    torch.save(model_vit,\n",
    "                                       './cass-vit-tuned.pt')\n",
    "                            \n",
    "        writer.flush()                \n",
    "        print('*'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Performance Summary\n",
    "#### Autoimmune Diseases Biopsy Slides Dataset\n",
    "\n",
    "- CASS Resnet-50\n",
    "  - F1 Score: 0.8621\n",
    "- CASS ViT B/16 g\n",
    "  - F1 Score: 0.8781\n",
    "\n",
    "#### Dermofit Dataset\n",
    "\n",
    "- CASS Resnet-50\n",
    "  - F1 Score: 0.7112\n",
    "- CASS ViT B/16 g\n",
    "  - F1 Score: 0.6675\n",
    "\n",
    "#### Brain Tumor MRI Dataset\n",
    "\n",
    "- CASS Resnet-50\n",
    "  - F1 Score: 0.9859\n",
    "- CASS ViT B/16 g\n",
    "  - F1 Score: 0.9211\n",
    "\n",
    "\n",
    "### Analysis\n",
    "- **Replication**: The replication of the study yielded results that are comparable to those presented in the original paper, affirming the robustness and reliability of the CASS model.\n",
    "- **Performance Across Datasets**: The model's effectiveness is highlighted across various datasets: in autoimmune disease biopsy analysis, it demonstrated improved F1 scores with 100% labeled data. In the Dermofit dataset, CASS outshone both supervised and other self-supervised methods, showcasing its proficiency in handling diverse skin lesion types. For the Brain MRI Classification dataset, the model showed a notable improvement in bringing the performance of CNNs and Transformers closer. In the challenging ISIC-2019 dataset, known for class imbalances and inconsistent images, CASS again proved superior, especially in scenarios with limited labeled data.\n",
    "- **Model's Strengths**: These results across different medical imaging datasets emphasize CASS's adaptability to varied image characteristics and its robustness in scenarios with limited labeled data.\n",
    "\n",
    "### Conclusion\n",
    "- The CASS model stands out as a significant advancement in the field of medical image analysis. Its ability to maintain high accuracy and efficiency across diverse and challenging datasets positions it as a powerful tool for AI-driven medical diagnostics, especially in conditions with sparse data availability.\n",
    "\n",
    "### Future Plan: Ablation Studies\n",
    "- **Objective**: These studies aim to identify the optimal training configurations for the CASS model, ensuring it is well-tuned for diverse medical imaging tasks.\n",
    "- **Focus Areas**:\n",
    "  - **Training Epochs**: Assessing the impact of varying the number of epochs on the model's accuracy and learning efficiency.\n",
    "  - **Optimizers**: Examining how different optimizer choices influence the model’s performance.\n",
    "  - **Batch Size**: Understanding the effects of batch size variations on the model's training dynamics.\n",
    "  - **Encoder Size**: Exploring the influence of different encoder sizes on the model’s capability to process and learn from medical images.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "## If you'd like to test the checkpoints, use the the below section for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jw/anaconda3/envs/ssl3/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold\n",
    "import math\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torchcontrib.optim import SWA\n",
    "from torchmetrics import Metric\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing and augmentation\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = ImageFolder('brain_tumor/Training', transform=data_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "val_dataset = ImageFolder('brain_tumor/Testing', transform=data_transforms)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case1: load pre-trained model from unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # label_num2str = {'0': 'glioma', '1': 'meningioma', '2': 'notumor', '3': 'pituitary'}\n",
    "    label_num2str = {0: 'glioma', 1: 'meningioma', 2: 'notumor', 3: 'pituitary'}\n",
    "    label_str2num = {'glioma': '0', 'meningioma': '1', 'notumor': '2','pituitary': '3'}\n",
    "    fl_alpha = 1.0  # alpha of focal_loss\n",
    "    fl_gamma = 2.0  # gamma of focal_loss\n",
    "    cls_weight =  [0.5, 0.5, 0.5, 0.5]\n",
    "    cnn_name='resnet50'\n",
    "    vit_name='vit_base_patch16_224'\n",
    "    seed = 77\n",
    "    num_classes = 4\n",
    "    batch_size = 16\n",
    "    t_max = 16\n",
    "    lr = 1e-3\n",
    "    min_lr = 1e-6\n",
    "    n_fold = 6\n",
    "    num_workers = 8\n",
    "    gpu_idx = 0\n",
    "    device = torch.device(f'cuda:{gpu_idx}' if torch.cuda.is_available() else 'cpu')\n",
    "    gpu_list = [gpu_idx]\n",
    "    CNN = True\n",
    "    VIT = False\n",
    "\n",
    "class MyF1Score(Metric):\n",
    "    def __init__(self, cfg, threshold: float = 0.5, dist_sync_on_step=False):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        self.cfg = cfg\n",
    "        self.threshold = threshold\n",
    "        self.add_state(\"tp\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"fp\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"fn\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        assert preds.shape == target.shape\n",
    "        preds_str_batch = self.num_to_str(torch.sigmoid(preds))\n",
    "        target_str_batch = self.num_to_str(target)\n",
    "        tp, fp, fn = 0, 0, 0\n",
    "        for pred_str_list, target_str_list in zip(preds_str_batch, target_str_batch):\n",
    "            for pred_str in pred_str_list:\n",
    "                if pred_str in target_str_list:\n",
    "                    tp += 1\n",
    "                if pred_str not in target_str_list:\n",
    "                    fp += 1\n",
    "\n",
    "            for target_str in target_str_list:\n",
    "                if target_str not in pred_str_list:\n",
    "                    fn += 1\n",
    "        self.tp += tp\n",
    "        self.fp += fp\n",
    "        self.fn += fn\n",
    "\n",
    "    def compute(self):\n",
    "        f1 = 2.0 * self.tp / (2.0 * self.tp + self.fn + self.fp)\n",
    "        return f1\n",
    "    \n",
    "    def num_to_str(self, ts: torch.Tensor) -> list:\n",
    "        batch_bool_list = (ts > self.threshold).detach().cpu().numpy().tolist()\n",
    "        batch_str_list = []\n",
    "        for one_sample_bool in batch_bool_list:\n",
    "            lb_str_list = [self.cfg.label_num2str[lb_idx] for lb_idx, bool_val in enumerate(one_sample_bool) if bool_val]\n",
    "            batch_str_list.append(lb_str_list)\n",
    "        return batch_str_list\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    The focal loss for fighting against class-imbalance\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1e-12  # prevent training from Nan-loss error\n",
    "        self.cls_weights = torch.tensor([CFG.cls_weight],dtype=torch.float, requires_grad=False, device=CFG.device)\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        logits & target should be tensors with shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        probs = torch.sigmoid(logits)\n",
    "        one_subtract_probs = 1.0 - probs\n",
    "        # add epsilon\n",
    "        probs_new = probs + self.epsilon\n",
    "        one_subtract_probs_new = one_subtract_probs + self.epsilon\n",
    "        # calculate focal loss\n",
    "        log_pt = target * torch.log(probs_new) + (1.0 - target) * torch.log(one_subtract_probs_new)\n",
    "        pt = torch.exp(log_pt)\n",
    "        focal_loss = -1.0 * (self.alpha * (1 - pt) ** self.gamma) * log_pt\n",
    "        focal_loss = focal_loss * self.cls_weights\n",
    "        return torch.mean(focal_loss)\n",
    "    \n",
    "cfg=CFG()\n",
    "val_metric=MyF1Score(cfg)\n",
    "\n",
    "fl_alpha = 1.0  # alpha of focal_loss\n",
    "fl_gamma = 2.0  # gamma of focal_loss\n",
    "criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
    "\n",
    "model_cnn = timm.create_model(cfg.cnn_name, pretrained=True)\n",
    "model_vit = timm.create_model(cfg.vit_name, pretrained=True)\n",
    "model_cnn.to(device)\n",
    "model_vit.to(device)\n",
    "model_vit=torch.load('./cass-vit.pt')\n",
    "model_cnn=torch.load('./cass-r50.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case2: load pre-trained model from downstream supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Validation Score: tensor(0.9859)\n"
     ]
    }
   ],
   "source": [
    "model_cnn=torch.load('./cass-r50-tuned.pt')\n",
    "model_cnn.eval()\n",
    "for images,label in val_loader:\n",
    "    images = images.to(device)\n",
    "    label = label.to(device)\n",
    "    model_cnn.to(device)\n",
    "    pred_ts=model_cnn(images)\n",
    "    label_one_hot = torch.nn.functional.one_hot(label, num_classes=4).float()\n",
    "    score_val = val_metric(pred_ts, label_one_hot)\n",
    "    val_loss = criterion(pred_ts, label_one_hot)  \n",
    "val_score=val_metric.compute()\n",
    "print('CNN Validation Score:',val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
